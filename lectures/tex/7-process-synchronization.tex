\documentclass[10pt,a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[scaled]{helvet}
\usepackage{cite}
\usepackage{url}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{float}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{color}
 
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{ %
  language=Octave,                % the language of the code
  basicstyle=\footnotesize,           % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  numberstyle=\tiny\color{gray},  % the style that is used for the line-numbers
  stepnumber=1,                   % the step between two line-numbers. If it's 1, each line 
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},      % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=true,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=none,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  keywordstyle=\color{blue},          % keyword style
  commentstyle=\color{dkgreen},       % comment style
  stringstyle=\color{mauve},         % string literal style
  escapeinside={\%*}{*)},            % if you want to add LaTeX within your code
  morekeywords={*,...}               % if you want to add more keywords to the set
}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{lastpage}
\floatstyle{boxed} 
\restylefloat{figure}
\renewcommand*\familydefault{\sfdefault}
\title{Synchronization}
\author{David Lynch - david.lynch@raglansoftware.com }
\begin{document}
\maketitle
\begin{abstract}
To date we have talked about concurrently running threads of execution, but have not addressed any of the fundamental protections that must be afforded to state that is shared between different threads. We have already alluded to a number of problems with data consistency and determinism that are surfaced by multi-threaded execution. This article examines these issues in detail and discusses some protections that operating systems provided to mitigate against these issues.
\end{abstract}
\section{The Critical Section}
Where several processes or threads access and manipulate the same data concurrently, and the output is dependent on the order in which the operations take place we have a {\bf race condition}. Synchronization and co-ordination is concerned with how interacting threads or processes can protect against the incorrect or inconsistent outputs this condition has the potential to produce. A {\bf critical section} is a segment of code in a process where the process changes some common resource or state. A protocol must exist whereby each process must request permission to enter the critical section. The point at which this protocol is used is known as the {\bf entry section}. The end of a critical section is demarcated by the {\bf exit section}. Trivially, the code that executes after the exit section is known as the {\bf remainder section}.
\subsection{Example}
Consider a Java program that simulates facilities in a bank. By their very nature, different facilities run concurrently. From the customer perspective, I must be able to withdraw money from my account at all times - even when other transactions are applied to my account at the same time. Figure \ref{prefix} shows the listing and figure \ref{results} shows the aggregated results.  We can clearly see there is a race condition here, as the results of this application are not consistent over multiple runs. Moreover, in unit testing, or otherwise ill-considered testing, this may be a tricky thing to spot. In fact, in general, debugging and designing multi-threaded applications is a tricky process that requires investment in time and an excellent understanding of multi-threading. 
\newline\newline
This race condition is facilitated by the scheduler. In particular, the lines that affect the results of the account. Figure \ref{bytecode} shows the byte-code that is generated by the line 10 in Account.java. Note that there are several distinct operations that are happening here, each of which is atomic at the operation level. Explicitly, the saving of the results of the operation back to the variable is a distinct operation from the addition of funds to the balance. Remember, threads will share a text-section. If the scheduler is switched out before the results of the operation have been written back, and the context switch results in a read of the balance, the resulting write-back may override the results of the original operation. To assist the scheduler, and prevent this operation from causing negative side-effects, we can instruct the JVM to make an operation {\bf atomic}. An atomic operation will either execute in full, or not execute at all. In Java, the {\bf synchronized} keyword is used to obtain a \{bf lock} on the shared state at the class level. This is the minimum level of locking in Java. More specifically, the synchronized functionality is implemented by means of a {\bf monitor}. Figure xx illustrates how a lock is obtained in Java. The curly braces indicate the scope of the critical section, with the first brace being the entry section and the final brace the exit section. We encounter much more detail on these concepts later. 
\newline\newline
It is extremely important that you understand everything that has happened here before moving on. With respect to race conditions in accesses to shared state, this is  as simple as it gets. Needless to say, things do, and will, get much more complex. Spend the time understanding everything from the actual problem that is being solved, the threads under execution and how they exist in the operating system to the Java byte code. 



First Figure = Prefix Listing

Third Figure = Postfix Listing

Second Figure = Results 
      1 1050.0
      1 170.0
      2 160.0
     96 1140.0
     
public void deposit(double);
  Code:
   0:	aload_0
   1:	dup
   2:	getfield	#15; //Field balance:D
   5:	dload_1
   6:	dadd
   7:	putfield	#15; //Field balance:D
   10:	return

\subsection{The Critical Section Problem}
We now move back from the implementation, to the abstract concepts at an OS level. We can generalise synchronization in Java to a critical section problem that needs to be solved by enforcing three principles.  
\subsubsection{Mutual Exclusion}
In any process critical sections can be drawn across threads internally and across relationships with other processes along a line that is connected with access to the same shared state. When we make a particular critical section mutually exclusive, then we ensure that no other process, or thread is executing either its copy of the critical section code, or code that will access shared state in a dangerous way concurrently. 
\subsubsection{Progress}
Mutual exclusion is simple. We can associate a marker or a {\bf lock} that can be acquired {\bf atomically} with the resource that we are intending to restrict access. However, we must provide a facility to ensure that these locks are acquired and released correctly, or otherwise a process may never proceed. In order to mitigate this, we enforce the principle of progress. Only processes that are not executing their remainder sections can participate in the decision for a process to enter its critical section, and this decision cannot be deferred indefinitely. 
\subsubsection{Bounded Waiting}
This is the final principle, and it states that there is a limit on the number of times that can enter its critical sections after a process has made a request to enter its critical section and before the request to enter is granted. This ensures, in some way, fairness. In simpler terms, if I wish to acquire a lock on a resource, any facility that solves the critical section problem, will guarantee that at some point, I will be able to acquire that lock. 
\subsubsection{Petersons Solution}
Peterson ?? proposed a solution to this problem that is illustrated by the listing in figure (peterson listing). The entry section guarantees mutual exclusion by ensuring that the process will not enter the critical section until it has been instructed to do so by the other process. Progress is guaranteed since when the process re-enters its critical section it immediately yields to other processes. Lastly, bounded waiting is enforced due to the fact that one execution of the section will happen at most before the critical section of another process is executed. 
\subsection{Semaphores & Mutexs}
A {\bf semaphore} is a means by which access to a shared resource can be restricted. The atomic operations {\bf wait} and {\bf signal} provide the interface to the semaphore. A count is maintained and is limited at the maximum number of entities that can hold the resource. If the maximum value of this count is 1, you will often here this primitive referred to as a {\bf mutex}. Figure illustrates a simple implementation of the semaphore. For two concurrently running processes with operation S1 for P1 and operation S2 for P2, we can ensure that S2 is only executed after S1 by making S2 wait on a lock from S1 and ensuring that S1 signals upon completion of its critical section. 
\subsection{Monitors}
Incorrect use of semaphores can result in problems that are difficult to detect. Semaphores rely on the discipline of the programmer to correctly invoke wait, signal and to maintain the correct order of execution. A {\bf Monitor} is a high level construct that remove the onus, and the risk of poor lock management from the programmer.  A monitor is an abstract data-type that presents public access to private data in a controlled manner. Typically a queue of interested parties is maintained. They are essentially equivalent to semaphores. In Java, each object is a monitor that exposes the {\it wait}, {\it notify} and {\it notifyAll} methods. These are used in conjunction with the synchronized method to facilitate concurrent programming Java. 
\section{Deadlock, Livelock and Starvation}
A semaphore with a waiting queue may result in a situation where two or more processes are waiting indefinitely for an event that can only be caused by one of the waiting processes in the queue. This situation is known as {\bf deadlock}. A very similar concept is {\bf livelock}, whereby the states of all of the members of the queue are constantly changing with respect one another, i.e. they are not waiting, but in the main, no progression is made. This is a special case of {\bf starvation}, whereby a process progression is hampered due to its inability to get a lock on the resource that it needs to progress, typically due to the resource being held by other processes at various times during its wait period. This typically happens if the queueing strategy is last-in-first out, and the queue never gets time to empty. 
\subsection{The Dining Philosophers Problem}
This is a well documented analogy that demonstrates the need to allocate resources in a deadlock free manner across competing resources. Philosophers  can be in one of two states, thinking or eating. They all sit at a round table and are sharing a bowl of rice. They cannot communicate with each other by thinking. When hungry, they may only acquire one chopstick at a time. When no longer hungry, each philosopher puts down both of his chopsticks. There are five chopsticks at a table of five philosophers, all sharing one bowl of rice. Each philosopher needs two chopsticks to eat. If there were no strategy to ensure progress, a deadlock situation could occur in a number of scenarios. If all philosophers become hungry at the same time, and each pick up the right chopstick first, there will be an indefinite wait on the right chopstick. This is a deadlock. If three philosophers are particularly hungry, they may acquire the chopsticks, leaving the other philosophers starving. If we restrict holding a single chopstick to a period of time, such that if we do not require a second chopstick in that time we release the single chopstick, then we are open to an interleaved wait-release cycle. This is a livelock. 
\subsection{The Bounded Buffer Problem}
This is also known as the producer consumer problem. There is a producer of work, a consumer of work and a shared queue. The producer and consumer both run concurrently. The consumer removes data from the queue, while the producer adds new work to the queue. We need to ensure that the producer does not try to add data to a full buffer. Similarly, the producer should not remove data from an empty queue. Lastly, we need to ensure that both processes do not deadlock or starve. The listing in figure x illustrates the problem. 
\section{Transactions and Atomicity}
A transaction is a single unit of work that is either performed in its entirity or not performed at all. If a transaction is completed in its entirity it is said to be {\bf committed}. If the transaction is not completed it is said to be aborted. To ensure atomicity, the effects of aborted transactions must be {\bf rolled back}. This ensures consistency by leaving the system in the same state as it appeared before it entered the transaction.  
\subsection{Log Based Recovery}
In order to support rollback, one approach to ensuring the rollback of changes to volatile memory is to log all modifications. Write-ahead logging uses log entries to record data about the modification. This will include a unique transaction id, the old value and the new value. Everything in this case is recorded to stable storage before volatile storage is modified. There is a performance penalty, the cost being more complexity and more storage costs. The advantage is the ability to undo the transaction by reversing the log, and transactions can be replayed  by playing the log forward. 
\subsection{Checkpoints}
The disadvangte of explicit logging is that replaying the log may be redundant for some or a lot of transactions. Determining which transactions need to be redone and which need to be rolledback is a search problem and can be costly. Checkpointing will set explicit safe points at which all records in volatile memory are outputted to stable storage. When just after a checkpoint is logged, everything is fully consistent. However, data can still be lost between checkpoints. 
\subsection{Concurrent Atomic Transactions}


\bibliography{../biblio/techfundamentals.bib}{}
\bibliographystyle{plain}
\begin{center}
{\small \copyright  David Lynch 2012. Do not reproduce without written permission.}
\end{center}
\end{document}
